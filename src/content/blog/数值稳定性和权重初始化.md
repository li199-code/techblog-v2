---
title: 数值稳定性和权重初始化
tags: [deep-learning]
categories: 动手pytorch
abbrlink: d686ee03
date: 2023-03-27 13:01:59
---

## 梯度爆炸和梯度消失问题

### 梯度爆炸

因为梯度的计算是通过偏导数的链式法则，所以，对于一个很深的网络，反向传播时，计算最后几层的梯度，很可能会超出数值的边界。比如cuda限制了16位的浮点数运算。这时，$1.1^100$超过了数值上界，程序就会报错。

### 梯度消失

如果某几个中间层的梯度很小，接近于0，那么前面几个层的梯度也是0，权重参数就得不到更新，这就是梯度消失。Sigmoid函数求导后，两头的梯度都接近于0，所以，很容易发生梯度消失。

## 合理初始化权重

合理初始化权重，可以缓解梯度爆炸和梯度消失问题。合理初始化化权重，就好比在选择一个离终点（最优点）近，且好走的起点。具体做法是，假设每层的输出和权重都满足独立正态分布，且均值、方差都相等。这种假设有以下几个原因(chatgpt)：

> 避免梯度消失或梯度爆炸：如果每一层的输出和权重具有相同的均值和方差，那么它们传递的梯度也会有相同的大小，这可以避免梯度消失或梯度爆炸问题。
>
> 提高网络的收敛速度：假设每一层的输出和权重具有相同的均值和方差，可以使网络更容易收敛，因为权重初始化过大或过小可能会使得网络的训练变得非常缓慢。
>
> 减少过拟合的可能性：如果权重初始化过大，网络容易出现过拟合现象，而假设每一层的输出和权重具有相同的均值和方差可以避免这种情况的发生。